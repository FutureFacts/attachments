{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cdaafb9",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# Using `attachments` with the OpenAI API\n",
    "\n",
    "This tutorial demonstrates how to use the `attachments` library to process local or remote files\n",
    "and prepare their content for use with the OpenAI API, particularly for multimodal models\n",
    "like GPT-4 with Vision or for text-based analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc3c8b6",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, ensure you have the `attachments` and `openai` libraries installed.\n",
    "\n",
    "```bash\n",
    "uv pip install attachments openai python-dotenv\n",
    "```\n",
    "\n",
    "Now, let's import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e622958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.11 environment at: /home/maxime/Projects/attachments/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m76 packages\u001b[0m \u001b[2min 338ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m13 packages\u001b[0m \u001b[2min 85ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.79.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.33.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install attachments openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6716a468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure you have an .env file with your OPENAI_API_KEY or set it as an environment variable\n",
    "import os\n",
    "from attachments import Attachments\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa19e771",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 2. Initialize Attachments\n",
    "\n",
    "We'll create an `Attachments` object. You can use URLs or local file paths.\n",
    "For this example, let's use a publicly available PDF and an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c46821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download content from URL: https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\n",
      "URL https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf has Content-Type: application/pdf; qs=0.001\n",
      "Successfully downloaded URL https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf to temporary file: /tmp/tmpz5ruyfo5.pdf\n",
      "Cleaned up temporary file: /tmp/tmpz5ruyfo5.pdf\n",
      "Attempting to download content from URL: https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/BremenBotanikaZen.jpg/1280px-BremenBotanikaZen.jpg\n",
      "URL https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/BremenBotanikaZen.jpg/1280px-BremenBotanikaZen.jpg has Content-Type: image/jpeg\n",
      "Successfully downloaded URL https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/BremenBotanikaZen.jpg/1280px-BremenBotanikaZen.jpg to temporary file: /tmp/tmpsvd5ghgy.jpg\n",
      "Cleaned up temporary file: /tmp/tmpsvd5ghgy.jpg\n"
     ]
    }
   ],
   "source": [
    "# Example using online resources\n",
    "pdf_url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/BremenBotanikaZen.jpg/1280px-BremenBotanikaZen.jpg\"\n",
    "\n",
    "# You can also use local paths, e.g.:\n",
    "# pdf_local_path = \"path/to/your/document.pdf\"\n",
    "# image_local_path = \"path/to/your/image.jpg\"\n",
    "# attachments_obj = Attachments(pdf_local_path, image_local_path, verbose=True)\n",
    "\n",
    "attachments_obj = Attachments(pdf_url, image_url, verbose=True) # Renamed to avoid conflict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61073e4",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 3. Inspecting Attachments\n",
    "\n",
    "The `Attachments` object processes the files. Its string representation (`str(attachments_obj)`)\n",
    "provides an XML-like format suitable for LLM prompts. For vision models,\n",
    "the `.images` property provides base64 encoded image data URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe53db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LLM Context String (sample) ---\n",
      "<?xml version=\"1.0\" ?>\n",
      "<attachments>\n",
      "  <attachment id=\"contact_sheet1\" type=\"jpeg\" original_path=\"[auto-generated contact sheet for https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf]\">\n",
      "    <content/>\n",
      "  </attachment>\n",
      "  <attachment id=\"pdf1\" type=\"pdf\" original_path=\"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\">\n",
      "    <content>Dummy PDF file\n",
      "\n",
      "</content>\n",
      "  </attachment>\n",
      "  <attachment id=\"jpeg2\" type=\"jpeg\" original_path=\"https://upload.wikimedia.org/w...\n"
     ]
    }
   ],
   "source": [
    "# Get the string representation for text-based analysis or context\n",
    "llm_context_string = str(attachments_obj)\n",
    "print(\"--- LLM Context String (sample) ---\")\n",
    "# Print a sample, as it can be very long\n",
    "print(llm_context_string[:500] + \"...\" if len(llm_context_string) > 500 else llm_context_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c905a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access image data for vision models\n",
    "# .images will contain a list of data URLs (e.g., \"data:image/jpeg;base64,...\")\n",
    "if attachments_obj.images:\n",
    "    print(f\"\\n--- Found {len(attachments_obj.images)} image(s) ---\")\n",
    "    # print(\"First image data URL (sample):\", attachments_obj.images[0][:100] + \"...\") # Print a sample of the data URL\n",
    "else:\n",
    "    print(\"\\n--- No images found or processed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afabfef",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 4. Preparing Content for OpenAI API\n",
    "\n",
    "Let's construct a message for the OpenAI API. We'll demonstrate a multimodal example\n",
    "using GPT-4o (or another vision-capable model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c63f92c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI() # Assumes OPENAI_API_KEY is set in your environment via .env or system variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414cc790",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 4.1. Multimodal Prompt (Text and Images)\n",
    "\n",
    "We'll combine the textual context from `str(attachments_obj)` with any images found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1b557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the content list for the OpenAI API\n",
    "openai_messages_content = []\n",
    "\n",
    "# Add text part: a general instruction and the context from attachments_obj\n",
    "prompt_text = f'''\n",
    "Analyze the following documents and images. Provide a brief summary of the PDF content\n",
    "and describe the image.\n",
    "\n",
    "Document context:\n",
    "{llm_context_string}\n",
    "'''\n",
    "openai_messages_content.append({\"type\": \"text\", \"text\": prompt_text})\n",
    "\n",
    "# Add image parts\n",
    "for image_data_url in attachments_obj.images:\n",
    "    # OpenAI API expects image_url with \"data:image/jpeg;base64,...\" format for base64 encoded images\n",
    "    openai_messages_content.append({\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": image_data_url,\n",
    "            \"detail\": \"low\" # Use \"high\" for more detail, \"low\" for faster processing\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a68ec",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### 4.2. Making the API Call (Example)\n",
    "\n",
    "Now, let's construct the full message and show how you would make the API call.\n",
    "**Note:** Running this cell will make an API call to OpenAI if your API key is configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a80477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to call OpenAI API (multimodal)...\n",
      "\n",
      "--- OpenAI API Response (Multimodal) ---\n",
      "## Summary of PDF Content\n",
      "\n",
      "The PDF document labeled as a \"Dummy PDF file\" appears to be a placeholder or sample PDF with minimal content. It is likely used for testing purposes rather than containing substantial information or data.\n",
      "\n",
      "## Description of the Image\n",
      "\n",
      "The image depicts a Zen garden, which features neatly raked gravel or sand creating a pattern of parallel lines. This style is typical of traditional Japanese dry landscape gardens, emphasizing simplicity and minimalism. It evokes a sense of tranquility and meditation.\n"
     ]
    }
   ],
   "source": [
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OPENAI_API_KEY not found in environment variables. Skipping API call.\")\n",
    "    print(\"Please create a .env file with OPENAI_API_KEY='your_key_here' or set it as an environment variable.\")\n",
    "else:\n",
    "    print(\"Attempting to call OpenAI API (multimodal)...\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\", # Or your preferred vision-capable model like \"gpt-4-turbo\"\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": openai_messages_content\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        print(\"\\n--- OpenAI API Response (Multimodal) ---\")\n",
    "        print(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API (multimodal): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af073cf2",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## 5. Text-Only Analysis\n",
    "\n",
    "If you are using a text-only model (e.g., `gpt-3.5-turbo`), you would only pass the `llm_context_string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d62700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting text-only OpenAI API call...\n",
      "\n",
      "--- OpenAI Text-Only API Response ---\n",
      "The main subject of the PDF is a \"Dummy PDF file\" as mentioned in the content of the attachment with id \"pdf1\".\n"
     ]
    }
   ],
   "source": [
    "# Example for a text-only model\n",
    "text_only_prompt = f'''\n",
    "Based on the following document content, please answer specific questions or perform tasks.\n",
    "For example, what is the main subject of the PDF?\n",
    "\n",
    "Document context:\n",
    "{llm_context_string}\n",
    "'''\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OPENAI_API_KEY not found. Skipping text-only API call.\")\n",
    "else:\n",
    "    print(\"\\nAttempting text-only OpenAI API call...\")\n",
    "    try:\n",
    "        response_text_only = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\", # Or your preferred text model\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text_only_prompt\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        print(\"\\n--- OpenAI Text-Only API Response ---\")\n",
    "        print(response_text_only.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API (text-only): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1469c",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial showed how to use the `attachments` library to load files/URLs,\n",
    "extract their content into formats suitable for LLMs, and construct prompts\n",
    "for the OpenAI API for both multimodal and text-only analysis.\n",
    "\n",
    "Remember to handle your API keys securely (e.g., using a `.env` file and `python-dotenv`)\n",
    "and manage costs associated with API calls. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "region_name,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
